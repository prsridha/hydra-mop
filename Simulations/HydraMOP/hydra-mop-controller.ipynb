{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bccfc694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import string\n",
    "import random\n",
    "import itertools\n",
    "import requests\n",
    "from pprint import pprint\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee07431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_hydra_job(epoch, worker, mg, mg_id):\n",
    "    print(\"Scheduling epoch {} of model_group {} with id {} on worker {}\".format(epoch, mg, mg_id, worker))\n",
    "    \n",
    "    exec_id = ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(32))\n",
    "    data = {\n",
    "        \"epoch\": epoch,\n",
    "        \"models\": str(mg),\n",
    "        \"exec_id\": str(exec_id),\n",
    "        \"model_group_id\": str(mg_id)\n",
    "    }\n",
    "    worker_ip = \"http://localhost:\" + str(8000 + worker) + \"/hydra\"\n",
    "    requests.post(url=worker_ip, json=data)\n",
    "\n",
    "    return exec_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0894af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_combinations(param_grid):\n",
    "    param_keys = list(param_grid.keys())\n",
    "\n",
    "    params_list = [param_grid[key] for key in param_keys]\n",
    "    combinations = list(itertools.product(*params_list))\n",
    "\n",
    "    param_combinations = []\n",
    "    for comb in combinations:\n",
    "        d = {}\n",
    "        for i in range(len(comb)):\n",
    "            d[param_keys[i]] = comb[i]\n",
    "        param_combinations.append(d)\n",
    "\n",
    "    return param_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0a990fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_groups(models, is_large_model, group_sz=2):\n",
    "    large_models = []\n",
    "    small_models = []\n",
    "    for i in range(len(models)):\n",
    "        if is_large_model[i]:\n",
    "            large_models.append(models[i])\n",
    "        else:\n",
    "            small_models.append(models[i])\n",
    "    \n",
    "    nlm = len(large_models)\n",
    "    nsm = len(small_models)\n",
    "    ngroups = int(math.floor(nlm / group_sz))\n",
    "    model_groups = []\n",
    "    \n",
    "    for i in range(ngroups):\n",
    "        if i == ngroups - 1:\n",
    "            model_groups.append(tuple(large_models[i*group_sz:]))\n",
    "        else:\n",
    "            model_groups.append(tuple(large_models[i*group_sz:(i + 1) * group_sz]))\n",
    "        \n",
    "    for i in range(nsm):\n",
    "        model_groups.append(tuple(small_models[i]))\n",
    "    \n",
    "    return model_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1fcbbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_workers(nodes, train_shard_path=\"root_path\"):\n",
    "    # create a virtual worker for every GPU in the cluster\n",
    "    worker_on_node = defaultdict(tuple)\n",
    "    single_workers = []\n",
    "    train_partitions = []\n",
    "    \n",
    "    worker_id = 0\n",
    "    for id, gpus in nodes.items():\n",
    "        ngpus = len(gpus)\n",
    "        if ngpus == 1:\n",
    "            single_workers.append(worker_id)\n",
    "        \n",
    "        for gpu_id in range(ngpus):\n",
    "            train_partitions.append(os.path.join(train_shard_path, \"gpu\" + str(gpu_id)))\n",
    "            worker_on_node[worker_id] = (id, gpu_id)\n",
    "            worker_id += 1\n",
    "\n",
    "    return worker_on_node, train_partitions, single_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4679f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_finished(worker, exec_id):\n",
    "    with open(\"check_finished_\" + str(worker) + \".txt\", \"r\") as f:\n",
    "        s = f.read().split(\"\\n\")\n",
    "    return exec_id in s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78715673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_epoch(n_workers, param_grid, is_large_model, group_sz=2):\n",
    "    initial_msts = find_combinations(param_grid)\n",
    "    \n",
    "    model_id_to_mst_mapping = {}\n",
    "    current_msts = [(mst_id, mst) for mst_id, mst in enumerate(initial_msts)]\n",
    "    for (mst_id, mst) in current_msts:\n",
    "        model_id_to_mst_mapping[mst_id] = mst\n",
    "\n",
    "    model_list = list(model_id_to_mst_mapping.keys())\n",
    "    model_groups = create_model_groups(model_list, is_large_model)\n",
    "    print(\"Model Groups:\", pprint(model_groups))\n",
    "    \n",
    "    s = \"Model ID: Model msts\\n\"\n",
    "    for i in range(len(models_list)):\n",
    "        s += str(models_list[i]) + \" : \" + pprint.pformat(initial_msts[i]) + \"\\n\"\n",
    "    print(\"Initial model configurations:\", s)\n",
    "    \n",
    "    model_group_nworkers_trained = []\n",
    "    model_group_on_workers = []\n",
    "    \n",
    "    for mgid, mg in enumerate(model_groups):\n",
    "        if len(mg) > 1:\n",
    "            model_group_on_workers.append(None)\n",
    "        else:\n",
    "            model_group_on_workers.append(None)\n",
    "        model_group_nworkers_trained.append(0)\n",
    "\n",
    "    mgw_pair = []\n",
    "    for mgid in range(len(model_groups)):\n",
    "        lis = []\n",
    "        for j in range(n_workers):\n",
    "            lis.append(False)\n",
    "        mgw_pair.append(lis)\n",
    "\n",
    "    worker_running_model_group = [None] * n_workers\n",
    "    exec_id_on_worker = [None] * n_workers\n",
    "    \n",
    "    return model_groups, model_group_on_workers, worker_running_model_group, mgw_pairs, model_group_nworkers_trained, exec_id_on_worker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "546ecabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idle_workers(worker_on_node, worker_running_model_group, single_workers, group_sz=2):\n",
    "    idle_worker_groups = set()\n",
    "    idle_worker_singles = set()\n",
    "    \n",
    "    for w, m in enumerate(worker_running_model_group):\n",
    "        if m == -1:\n",
    "            idle_worker_singles.add((w, ))\n",
    "    \n",
    "    for i in idle_worker_singles:\n",
    "        for j in idle_worker_singles:\n",
    "            if i >= j:\n",
    "                continue\n",
    "            # if 2 workers belong to the same node, only then group together.\n",
    "            if worker_on_node[i][0] == worker_on_node[j][0]:\n",
    "                idle_worker_groups.add((i, j))\n",
    "    \n",
    "    # add single GPU nodes to idle_worker_groups\n",
    "    for w in single_workers:\n",
    "        if (w,) in idle_worker_singles:\n",
    "            idle_worker_groups.add((w,))\n",
    "    \n",
    "    return idle_worker_groups, idle_worker_singles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b71fdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_runnable_mgw_pair(idle_worker_groups, idle_worker_singles, model_groups, model_group_on_workers, mgw_pairs):    \n",
    "    runnable_mgw_pair = None\n",
    "    random.shuffle(model_groups)\n",
    "\n",
    "    for mgid, mg in enumerate(model_groups):\n",
    "        # mg is running on some other worker(s)\n",
    "        if model_group_on_workers[mgid] != None:\n",
    "            continue\n",
    "\n",
    "        if len(mg) > 1:\n",
    "            # large model group\n",
    "            for wg in idle_worker_groups:\n",
    "                # check if mg has already been trained on any worker part of wg.\n",
    "                trained_on_wg = False\n",
    "                for w in idle_worker_groups:\n",
    "                    if mgw_pairs[mgid][w] == True:\n",
    "                        trained_on_wg = True\n",
    "                        break\n",
    "                # we found our runnable model_group worker_group pair\n",
    "                if not trained_on_wg:\n",
    "                    runnable_mgw_pair = (mgid, wg)\n",
    "                    return runnable_mgw_pair\n",
    "\n",
    "        else:\n",
    "            # small model group\n",
    "            for wg in idle_worker_singles:\n",
    "                w = wg[0]\n",
    "                if mgw_pairs[mgid][w] == False:\n",
    "                    runnable_mgw_pair = (mgid, wg)\n",
    "                    return runnable_mgw_pair\n",
    "\n",
    "    return runnable_mgw_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b48f5365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_status():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03375fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a16a537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(self, epoch, workers, model_groups, train_partitions, valid_partitions, virtual_worker_on_worker, model_id_to_mst_mapping, model_group_on_worker,\n",
    "              mgw_pair, exec_id_on_worker, model_groups_nworkers_trained, worker_running_model_group):\n",
    "        \n",
    "    n_virtual_workers = len(virtual_worker_on_worker)\n",
    "\n",
    "#         model_id_ckpt_mapping = self.get_model_on_checkpoint()\n",
    "\n",
    "    model_groups_to_build = set(range(len(model_groups)))\n",
    "\n",
    "    controller_logger.info(\"Beginning model scheduling...\")\n",
    "    model_worker_logger.info(\"Starting epoch...\")\n",
    "\n",
    "    while (len(model_to_build) > 0):\n",
    "        for vw in range(n_virtual_workers):\n",
    "            # get the real worker and gpu associated with virtual worker vw\n",
    "            worker_id = virtual_worker_on_worker[vw][0]\n",
    "            gpu_id = virtual_worker_on_worker[vw][1]\n",
    "            # model_worker_logger.info(str((vw, worker_running_model[vw])))\n",
    "            if worker_running_model[vw] == -1:\n",
    "                m = self.get_runnable_model(vw, models_list, model_on_worker, mw_pair)\n",
    "                if m != -1:\n",
    "                    is_last_worker = model_nworkers_trained[m] == n_virtual_workers - 1\n",
    "                    exec_id = self.launch_job(epoch, worker_id, gpu_id, workers[worker_id],\n",
    "                                              train_partitions[vw],\n",
    "                                              valid_partitions,\n",
    "                                              m,\n",
    "                                              model_id_ckpt_mapping[m],\n",
    "                                              train_fn_string,\n",
    "                                              valid_fn_string,\n",
    "                                              model_id_to_mst_mapping[m],\n",
    "                                              is_last_worker,\n",
    "                                              kwargs_str\n",
    "                                            )\n",
    "                    model_on_worker[m] = vw\n",
    "                    worker_running_model[vw] = m\n",
    "                    exec_id_on_worker[vw] = exec_id\n",
    "\n",
    "                    self.set_model_on_worker(model_on_worker)\n",
    "                    self.set_worker_running_model(worker_running_model)\n",
    "                    self.set_execid_on_worker(exec_id_on_worker)\n",
    "\n",
    "                    print(\"Sent model {} to build on worker {} on GPU {} with config {}\".format(\n",
    "                        str(m), str(worker_id), str(gpu_id), str(model_id_to_mst_mapping[m])))\n",
    "                    model_worker_logger.info(\"Sent model {} to build on worker {} on GPU {} with config {}\".format(\n",
    "                        str(m), str(worker_id), str(gpu_id), str(model_id_to_mst_mapping[m])))\n",
    "            else:\n",
    "                # poll since this particular worker is busy\n",
    "                m = worker_running_model[vw]\n",
    "                if m != -1:\n",
    "                    exec_id = exec_id_on_worker[vw]\n",
    "                    completed, status = self.check_finished(workers[worker_id], exec_id)\n",
    "\n",
    "                    if completed:\n",
    "                        print(\"Received Model {} built on worker {} on GPU {}\".format(str(m), str(worker_id), str(gpu_id)))\n",
    "                        model_worker_logger.info(\"Received Model {} built on worker {} on GPU {}\".format(str(m), str(worker_id), str(gpu_id)))\n",
    "                        # models[m].n = status[\"result\"]\n",
    "                        model_on_worker[m] = -1\n",
    "                        worker_running_model[vw] = -1\n",
    "                        exec_id_on_worker[vw] = None\n",
    "                        model_nworkers_trained[m] += 1\n",
    "                        mw_pair[m][vw] = True\n",
    "                        model_done = True\n",
    "                        for i in range(n_virtual_workers):\n",
    "                            if not mw_pair[m][i]:\n",
    "                                model_done = False\n",
    "                                break\n",
    "                        if model_done:\n",
    "                            model_to_build.remove(m)\n",
    "\n",
    "                self.set_model_on_worker(model_on_worker)\n",
    "                self.set_worker_running_model(worker_running_model)\n",
    "                self.set_model_worker_pairs(mw_pair)\n",
    "                self.set_model_nworkers_trained(model_nworkers_trained)\n",
    "                self.set_execid_on_worker(exec_id_on_worker)\n",
    "\n",
    "            # TODO: write out execution order in standard format: and also replay schedule(to replay any given scheduler)\n",
    "            sleep(1)\n",
    "    model_worker_logger.info(\"Ending epoch...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c21f046",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model_runtime = 5\n",
    "large_model_runtime = 20\n",
    "nlarge = 11\n",
    "nsmall = 5\n",
    "\n",
    "is_large_model = ([True]*nlarge) + ([False]*nsmall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a62e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = {0: [\"GPU0\", \"GPU1\",\"GPU2\",\"GPU3\"],\n",
    "           1: [\"GPU0\", \"GPU1\", \"GPU2\"],\n",
    "           2: [\"GPU0\", \"GPU1\"],\n",
    "           3: [\"GPU0\", \"GPU1\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ddd735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "        'learning_rate': [1e-2, 1e-3],\n",
    "        'embed_size': [256, 512],\n",
    "        'hidden_size': [256, 512],\n",
    "        'batch_size': [128, 256]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b57f2a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search():\n",
    "    num_epochs = 2\n",
    "        \n",
    "    for i in range(num_epochs):\n",
    "        print(\"EPOCH: \" + str(i+1))\n",
    "        model_group_num_map, model_group_on_worker, worker_running_model_group, mgw_pairs = init_stuff()\n",
    "        scheduler(i, model_group_num_map, workers, model_group_on_worker, mgw_pairs, worker_running_model_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f87044a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2404a1d-1c7f-4551-bdf3-22b293d81b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaa069e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = set((1,2,3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f104d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = set((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e6d3955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3, 4}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddf46e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1cd345",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
